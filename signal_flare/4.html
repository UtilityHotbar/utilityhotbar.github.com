<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Program for a Different Model of AI Development</title>
    <link rel="stylesheet" href="essays.css">
</head>
<body>
    <div class="back-button">
        <em>Navigation</em><br>
        <hr>
        <a href="index.html">Home</a><br><a href="3.html">Previous</a><br><a href="5.html">Next</a>
    </div>
    <h1 id="a-program-for-a-different-model-of-ai-development">A Program for a Different Model of AI Development</h1>
<p><em>In which I ask for a program of human augmentation, rather than human
replacement—&ldquo;Good enough&rdquo; AI is a scam—A call to arms</em></p>
<h2 id="-1-a-note-on-accuracy-and-links">-1. A Note on accuracy and links</h2>
<p>All links to papers or resources present here are drawn from my
(limited) memory and do not represent the best or most well-written
works in the areas that I reference. While I am drawing on three years
of hands-on experience with models big and small as well as working on a
<u><a href="https://arxiv.org/abs/2302.10329">paper</a></u> and a
<u><a href="diss.html">dissertation</a></u> on this topic, I am nevertheless conscious
of the likelihood that I have made many obvious and non-obvious errors.
Factual and conceptual corrections are welcome where supplied with
sufficiently clear evidence. Contact me at mutatismutandisetplusultra at
gmail dot com.</p>
<h2 id="0-navigating-this-document">0. Navigating this document</h2>
<p>If you want an overview of why I believe the present mode of AI
development is harmful, start from the beginning with section 1. If you
are already convinced or just want to see some code, go to section 4. If
you are not particularly technically inclined, reading up to section 4
gives you a good idea of my general points and stance. If you are
interested in AI safety and feel pessimistic about the state of AI, I
would recommend everything, but especially section 4 onwards.</p>
<h2 id="1-introduction">1. Introduction</h2>
<p>The present state of AI development is dangerous and unsustainable.
Unfathomable amounts of time, manpower,
<u><a href="https://www.independent.co.uk/climate-change/news/google-climate-emissions-ai-data-centers-b2572865.html">electricity</a></u>,
and
<u><a href="https://gizmodo.com/microsoft-water-usage-ai-iowa-data-center-1850826419">water</a></u>
have been spent on models that promise the world but deliver
questionable results. The present <u><a href="diss.html?#the-narrative-of-ai">AI investment cycle</a></u>
depends on companies announcing a never-ending parade of new models with
powerful capabilities to attract new investment for training yet more
models. With revenue lagging far behind expenditure, this cycle creates
both a <u><a href="https://www.wheresyoured.at/burst-damage/">vicious bubble about to
burst</a></u> and dangerous race
dynamics between companies that encourage risk-taking, one-upmanship,
and dangerous deployments of unready technology.</p>
<p>Despite this, AI technology is definitely far more advanced than it was
even five (much less ten) years ago. The claims of the AI skeptics (that
AI would never master language or complex problem solving outside of
games) have been decisively wounded. As companies rush to slap labels
like “safe” and “friendly” on their models, the time will come when the
line between truly capable artificial intelligence and billions of
dollars of emergency hotfixes becomes invisible to the human eye.</p>
<p>I believe we are in need of a different method of conceiving of how to
use, deploy, and manage AI technology. This is my attempt to provide
that alternative view.</p>
<h2 id="2-ai-as-signal-processing">2. AI as Signal Processing</h2>
<p>The present paradigm of AI focuses on using AI as an advanced form of
signal processing. Raw data (images, user speech, video, natural
language prompts), too complex and multifaceted to be handled by human
programming or hand-crafted rules, comes into a black box called an “AI
model”. Processed data comes out of the black box in the form of image
labels, continuations of text, or entity classifications, in a format
that is legible and useful to humans. In other words, the image of a cat
already contains the information “this is a cat”, what AI does is
extract this “relevant” information from the image and discard the rest
<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>.</p>
<p>This problem is not new: signal processing is a discipline as old as
World War II, where separating a voice or electrical signal from the
noise generated through the physical medium a signal traveled in was key
<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>. The major innovation of the “machine learning revolution” has been
the development of ways to relieve humans of designing the processes
which perform the intermediate steps of signal processing. By providing
automated feedback to a system capable of <u><a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">approximating <em>any</em> signal
processing
function</a></u>,
we hope to steer it to organically settling upon an optimal algorithm
for our desired signal processing problem, whether that problem is
sorting the wheat from the chaff in a pile of hopeful applicants or
finding the lowest-error continuation of the signal we are receiving.</p>
<p>A major feature of this paradigm is that the intermediate steps of
computation (where the real work of “feature extraction” i.e. “signal
amplification” gets done) are efficiently abstracted away. The general
contemporary machine learning approach is to encourage the model to
develop an internal “shorthand” for encoding all manner of possible
inputs, and then asking it to kindly throw that shorthand away to expose
the few morsels of information we actually need <sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>. Thus, the vast
majority of information in an image of a cat (a tabby cat, incidentally,
sitting next to a potted plant, slightly frowning, with a crooked
whisker or two) is discarded to obtain an image embedding <sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup>, which is
itself crunched down to a list of human-specified labels with
probabilities listed next to them (cat: 50%, dog: 4%, gerbil: 2% …)
<sup id="fnref:5"><a class="footnote-ref" href="#fn:5">5</a></sup>. Incidentally, this is also how transformers work: a sequence of
tokens (“to”, “be”, “or”, “not”, “to”) come in, a great deal of
computation occurs where information is extracted and transferred
between tokens to create a contextual shorthand for the meaning of the
token sequence, then all that shorthand is discarded to obtain a list of
next-word probabilities: (“be”: 95%, “hamlet”: 1% …).</p>
<p>There are several problems with this approach. Perhaps the easiest to
explain is that a great deal of computation is wasted when this is done,
since all the animal-identifying model has learned about a given image
of a tabby cat is discarded when the next image (possibly only different
from the first by a few pixels!) comes in. This is part of the reason
why it is incredibly wasteful to run AI models, especially models with
high levels of information processing or transfer capabilities like the
transformers. Work is underway to cache latent representations, but as I
understand it, it is still in its early stages.</p>
<p>Perhaps more worryingly, such an efficient abstraction of inner
computational processes hides those processes both from our
<em>understanding</em> and from our <em>control</em>. If we trust the model to
separate signal from noise, we naturally cede control over the
definitions of “signal” and “noise” to the model. Has the model declared
this candidate a “good fit” for our company because he says he has a
strong work ethic, or because he is of the same ethnicity and cultural
background as the rest of the team? Who knows? It does good enough on
our self-defined benchmarks, so we should start pitching it to venture
capital!</p>
<p>An entire instrumental science of model interpretability has thus arisen
to research powerful AI models after training is complete, prying open
their internal shorthand and their learned parameters to extract
features or signals models focus on. Many
<u><a href="https://github.com/TransformerLensOrg/TransformerLens">tools</a></u>
and <u><a href="https://jalammar.github.io/illustrated-transformer/">guides</a></u>
have been published to this end, forming a small discipline of
interpretability research that is also sponsored by major developers
like OpenAI and
<u><a href="https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html">Anthropic</a></u>.
Leaving aside the sense that this is like inviting the vet to visit
after the horse has already left the barn, the pragmatic truth is that
this process is incredibly difficult. Models, besides basic constraints
of computing resources allocated and time spent on training, have no
impetus to make their decision making processes compact or <u><a href="https://arxiv.org/abs/2210.01892">human
legible in any way</a></u>. It also
remains unclear how, if a harmful feature were indeed discovered in a
production model, a model might be modified to “forget” such features
<sup id="fnref:6"><a class="footnote-ref" href="#fn:6">6</a></sup>. And, of course, the release of open weights models like <u><a href="https://llama.meta.com/">Llama
by Meta</a></u> means that even if such a
foolproof technique were discovered, we would still need to do a
“recall” on every AI model being used in the wild, which is somewhat
akin to trying to recall every radio in the world if duplicating radios
was also free.</p>
<h2 id="3-good-enough-ai">3. “Good Enough” AI</h2>
<p>With no fundamental way yet discovered to understand and modify AI
models from the inside, the “solution” settled on by OpenAI et al. is
simple: more training. Such paradigms have names like <u><a href="https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback">Reinforcement
Learning from Human
Feedback</a></u>
or <u><a href="https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback">Constitutional
AI</a></u>,
and largely consist of further feedback to the models that they have
already trained, this time of the variety “could you please not tell
people on the internet how to make methamphetamine, thanks”. In the days
of cat-dog identification, this training was known as “adversarial
robustness”: Both paradigms consist of further training to teach a model
to identify a class of signals that is pathological and therefore should
be ignored or handled in some special way. Unfortunately, we’re no
better at telling AI models not to label <u><a href="https://www.sciencedirect.com/science/article/abs/pii/S1383762124000924">bursts of TV static as
gibbons</a></u>
than we are at <u><a href="https://arxiv.org/html/2307.15043v2">telling ChatGPT not to teach murderers how to dispose
of bodies</a></u>. These companies
will pretend that they have no choice, that it is these stop-gap methods
or nothing. That is a lie. <strong>They always have the option of not
deploying or developing these models. They just choose to deploy them
anyways.</strong></p>
<p>So what are we left with? I do not claim that <em>no</em> progress has been
made in AI safety or AI alignment with “human values”. ChatGPT or GPT-4o
is demonstrably safer than GPT-3 out of the box when faced with most
harmful prompts <sup id="fnref:7"><a class="footnote-ref" href="#fn:7">7</a></sup>. However, as AI deployments become more
<u><a href="https://preview.devin.ai/">sophisticated</a></u> and more
<u><a href="https://arstechnica.com/information-technology/2024/08/research-ai-model-unexpectedly-modified-its-own-code-to-extend-runtime/">autonomous</a></u>,
I am more and more convinced that such incremental improvements do
nothing except create a false sense of security around these opaque and
obfuscated models, especially as the arms race to train GPT-5 and its
equivalents is already on and models are set to become <em>more</em> complex,
not less. <u><a href="https://situational-awareness.ai/">A general</a></u> is much
more likely to launch a 70% accurate guided missile than he is to launch
a 20% accurate missile, but the potential harm from a misguided missile
is still catastrophic (and has possibly become more severe now that
launching is no longer off the table).</p>
<p>What the major AI corporations are trying to sell us is AI that is “good
enough”, where they define “good” and they define “enough”. Well, fuck
that. If we must develop these models, can we at least do them in a
slightly less insane way?</p>
<h2 id="4-the-promise-of-a-better-ai">4. The Promise of a Better AI</h2>
<p>I am by no means a Luddite. I believe in the capacity for powerful
information processing technologies to make human lives and decision
making better. I believe that AI models, when used in properly
constrained cases, constitute powerful information processing and signal
processing systems. However, I believe that any such technology must
meet several key criteria.</p>
<ol>
<li>
<p><strong>Interpretability by design:</strong> Models must have at least somewhat
    human auditable and editable inner latent representations, even at
    the cost of performance. Post-hoc “alignment” is not enough.
    Ideally, these models should clearly and unambiguously showcase the
    strengths and downsides of any given model.</p>
</li>
<li>
<p><strong>Domain limitations:</strong> Models should do one thing well, not many
    things nebulously good enough.</p>
</li>
<li>
<p><strong>Efficiency:</strong> Models should not re-perform computation on already
    processed items, not only because it’s more efficient, but because
    it makes models more consistent over time and (I believe) ultimately
    more capable and reliable.</p>
</li>
<li>
<p><strong>Human in the loop:</strong> Current models are designed to be “agents”
    i.e. take the place of whole human workers at once. Models should
    instead aim to <u><u><a href="https://ethics.harvard.edu/how-ai-fails-us">enhance human decision
    making</a></u></u> and
    create opportunities for humans to be more creative, dynamic, and
    efficient.</p>
</li>
</ol>
<p>Below are three concepts for AI models (along with code, as a sign of
good will) that can demonstrate some of these ideas. In many of these
cases we do not swear off frontier models, but instead carefully scope
their deployment to maximise interpretability and human control.</p>
<h3 id="41-concept-one-unsupervised-human-friendly-online-object-categorisation">4.1 Concept One - Unsupervised Human-Friendly Online Object Categorisation</h3>
<ul>
<li><u><u><a href="https://github.com/utilityhotbar/sam2_hierarch">https://github.com/utilityhotbar/sam2_hierarch</a></u></u></li>
</ul>
<p>We use
<u><a href="https://github.com/facebookresearch/segment-anything-2">SAM2</a></u> to
segment incoming images into objects. Each object is then masked out and
fed into
<u><a href="https://huggingface.co/transformers/v4.8.0/model_doc/clip.html">CLIP</a></u>
to create embeddings of each object. Instead of discarding these
embeddings, we save them (alongside their associated image) in
automatically generated categories by clustering them following a
simplified <u><a href="https://arxiv.org/pdf/1909.09667">Online Hierarchical Agglomerative
Clustering</a></u> (OHAC) algorithm, with
the similarity index being the cosine similarity of the stored image
embeddings. As a result of this approach, the dynamically generated
classifications can be displayed as a list of folders containing images
of objects. Moving an image from one folder to another and then updating
the categories automatically gives us unprecedented control over the
model’s learned behaviour without further retraining.</p>
<h3 id="42-concept-two-textual-knowledge-graph-generation-and-analysis">4.2 Concept Two - Textual Knowledge Graph Generation and Analysis</h3>
<ul>
<li><u><u><a href="https://github.com/UtilityHotbar/autoratiocinator/">https://github.com/UtilityHotbar/autoratiocinator/</a></u></u></li>
</ul>
<p>Instead of feeding an LLM foundation model raw text and then prompting
it to summarise the text, we use foundation models to rewrite the text
to contain less ambiguity (pronouns, references etc) and then split the
text into sentences. These rewritten sentences are exposed and
auditable/editable by humans. Each sentence is compared against a list
of candidate related sentences using a cosine similarity heuristic. The
result is a knowledge graph that contains directed edges (sentence X
implies sentence Y) and undirected edges (sentence A is related to
sentence B). Using these edges, analyses can be provided for each
sentence in the text that are more auditable than simply prompting an
LLM with a large context window.</p>
<h3 id="43-concept-three-constraintclassification-separation">4.3 Concept Three - Constraint/Classification Separation</h3>
<p>Instead of retraining <u><a href="https://arxiv.org/abs/2111.06377v2">Masked Autoencoder-Vision Transformer
models</a></u> to make them “more
robust” against pathological inputs, I use the otherwise discarded
decoder from the training process as a way of testing if images have
been tampered with or are otherwise poorly represented within the latent
space of the vision transformer. If the reconstruction loss after an
image is fed through the constraint module (an encoder-decoder process)
is too high, the system will flag it as anomalous instead of attempting
to classify it. This is an example of treating machine learning models
as powerful contextual heuristics rather than general “world modelers”.</p>
<p>The proof of concept code is short enough to put here:</p>
<pre><code class="language-python">from transformers import ViTForImageClassification, AutoImageProcessor, ViTMAEForPreTraining
from datasets import load_dataset
# import tensorflow as tf
from PIL import Image

import pickle
import torch

dataset = load_dataset(&quot;huggingface/cats-image&quot;)

image = dataset[&quot;test&quot;][&quot;image&quot;][0]

image_processor = AutoImageProcessor.from_pretrained(&quot;facebook/vit-mae-base&quot;)

pretraining_model = ViTMAEForPreTraining.from_pretrained(&quot;facebook/vit-mae-base&quot;)

classifier_image_processor = AutoImageProcessor.from_pretrained(&quot;google/vit-base-patch16-224&quot;)

classifier_model = ViTForImageClassification.from_pretrained(&quot;google/vit-base-patch16-224&quot;,)

def safe_classify_image(image):
    inputs = image_processor(images=image, return_tensors=&quot;pt&quot;)

    outputs = pretraining_model(**inputs)

    loss = outputs.loss

    reconstructed_img = pretraining_model.unpatchify(outputs.logits).detach().cpu()

    print(reconstructed_img.shape)
    # Reject the input if its too different
    print(loss)
    if loss &gt; 0.5:
        return &quot;adversarial&quot;

    with torch.no_grad():
        classifier_inputs = classifier_image_processor(images=image, return_tensors=&quot;pt&quot;)
        logits = classifier_model(**classifier_inputs).logits

    predicted_label = logits.argmax(-1).item()

    final_label = classifier_model.config.id2label[predicted_label]
    return final_label

def raw_classify_image(image):
    classifier_inputs = classifier_image_processor(images=image, return_tensors=&quot;pt&quot;)

    with torch.no_grad():

        logits = classifier_model(**classifier_inputs).logits

    predicted_label = logits.argmax(-1).item()

    final_label = classifier_model.config.id2label[predicted_label]
    return final_label

if __name__ == '__main__':
    norm_label = raw_classify_image(image)
    safe_label = safe_classify_image(image)
    print(norm_label, safe_label)
</code></pre>
<h2 id="5-why-this-programmethodologyplan-for-safe-ai">5. Why this program/methodology/plan for safe AI?</h2>
<p>I know that my methods are not “sophisticated” in ML terms or backed by
years of research. However, this technical program was not borne out of
a grand theoretical ideal, or some utopian dream of benevolent AI
assistants for everyone. It was instead developed out of necessity due
to my limited access to funds and compute. With only a laptop and public
models to work with, I wanted to make systems that could function on
(relatively) limited hardware and still deliver human-interpretable and
editable inner representations. In that regard, I would argue that I
have succeeded more than many high-concept labs that have raised more
funds but produced much less to show for it.</p>
<p>Time is of the essence. The pieces of a world we do not wish to see are
being assembled around us as we speak.</p>
<h2 id="6-but-ai-isnt-actually-sentientthinkingself-aware-so-isnt-this-safety-stuff-all-just-hype-for-ai-companies">6. But AI isn’t actually sentient/thinking/self aware, so isn’t this safety stuff all just hype for AI companies?</h2>
<p>In some sense, I could not care less if AI was actually sentient or self
aware. LLMs and generative AI models, while definitely <em>not</em> sentient,
are already causing great losses to freelance artists and writers. If AI
models become more capable and are then placed in greater <u><a href="https://www.politico.eu/article/dutch-scandal-serves-as-a-warning-for-europe-over-risks-of-using-algorithms/">positions
of
responsibility</a></u>,
whether they are “really AI” or not becomes a moot point compared to the
harm they can cause <sup id="fnref:8"><a class="footnote-ref" href="#fn:8">8</a></sup>.</p>
<h2 id="7-wont-ai-capabilitiessafety-just-get-better-and-make-all-your-concerns-moot">7. Won’t AI capabilities/safety just get better and make all your concerns moot?</h2>
<p>I have strong reasons to believe that the limitations of current AI
models are tricky to solve without exponential investment in scaling,
which current funders may be <u><a href="https://www.wheresyoured.at/burst-damage">loathe to
provide</a></u>. I also believe that
<em>much of their supposed capacity</em> to handle knowledge work tasks is
either “patched in” through fine tuning or competence attributed to
vague language. Here I refer to large language models specifically
rather than AI models in general.</p>
<p>The reason I believe this limit exists is as follows: <u><a href="https://arxiv.org/html/2406.08787v1">Function
composition</a></u> is a
much-discussed topic with regards to the fundamental abilities of LLMs.
In general, function composition refers to the ability to apply logical
operations, mathematical functions etc. in a predefined sequence,
“composing” them into a longer function. A simple example is as follows:
<em>The cat ate the bat. The whale ate the cat. What ate the thing that ate
the bat?</em>. The question can be formulated as
<code>what_ate(what_ate("bat"))</code>, and solving the problem requires applying
both operations in sequence.</p>
<p>I have designed a <u><a href="https://github.com/UtilityHotbar/crashtestbench">rudimentary benchmark</a></u> to test LLM
capabilities to solve this category of problem at various depths, and
the initial results seem to indicate that they are no better than chance
at solving problems humans can be trained to solve trivially <sup id="fnref:9"><a class="footnote-ref" href="#fn:9">9</a></sup>. This
is reinforced by <u><a href="https://arxiv.org/html/2406.02061v1">existing
research</a></u>. I further believe
that many knowledge work tasks (generating policy briefs, summarising
text etc.) contain implicit forms of such function composition problems
(“Who is the protege of the last French prime minister?” etc.).
Currently LLM models might get around this using memorised facts in the
feed forward networks of each block, allowing them to hard-code answers
to such composition questions without needing to actually engage with
the actual logic problem. However, in tasks like autonomous scientific
research, autonomous policy briefing etc. these techniques will quickly
become useless due to the granularity of the issues at hand. It also
seems true that this limitation arises from <u><a href="https://arxiv.org/html/2402.08164v2">a limitation in the
fundamental design of current generation
transformers</a></u>. Therefore, I
have cause to believe that without exponential investment in scaling
infrastructure this hurdle may not be surpassed, while model
misbehaviour remains an issue.</p>
<p>As an add-on to all of this, I still believe in the original arguments
against language models—in essence, that the map is not the territory,
and that a <u><a href="https://blog.dileeplearning.com/p/ingredients-of-understanding">world model predicated on
language</a></u>
is an imperfect map onto a sensory reality powered by physics. However,
now I am quite satisfied that models will be able to map one onto the
other well enough to cause significant mischief.</p>
<h2 id="8-but-agiasithe-singularity-is-inevitable-so-why-bother">8. But AGI/ASI/the Singularity is inevitable, so why bother?</h2>
<p>Here I address the views present in articles like
<u><a href="https://gwern.net/tool-ai#why-you-shouldnt-be-a-tool">these</a></u>, as
well as arguments that because of some combination of technological and
social factors generalised AI systems are inevitable. Therefore, it is
implied that non-general efforts to create “tool-like” or “safe” AI
systems are wastes of time. Ironically, such arguments are often made
(in eerily similar ways) by both AI enthusiasts and AI safety
proponents, although in one case it is a cause for celebration and in
the other a cause for nihilism.</p>
<p>First, I will address the idea that AGI/agentic AI emergence is
inevitable because of race dynamics/market
dynamics/competition/capitalism etc. I believe that these arguments make
the mistake of correctly recognising the context of how we develop AI
today but then treating it as an inevitable constant. It is not a
guarantee that we will continue down this socio-political trajectory,
with endless competition leading to misery for all. Indeed, part of my
goal with human-auditable AI systems is to make social coordination and
therefore a better social system more likely, reducing the risk of
runaway race dynamics leading to agentic emergence.</p>
<p>Second, I believe that these claims establish some teleological endpoint
to AI development and then “handwave” the interim stages, often through
references to rapid takeoff etc., which I believe is a poor method of
advancing an argument. Simply because the underlying drives for a
hoped-for or unhoped-for outcome are present does not then relieve you
of the responsibility of ushering in the future you hope for or trying
to avert one you dread. Microsoft, Amazon, Facebook, and Google are
investing billions of dollars into AI development, committing huge
amounts of resources and manpower in the process. This is by no means a
done deal. At the risk of repeating the conclusion of my dissertation:</p>
<p><em>This ultimate truth of human responsibility extends to any of the
interpretations I have presented for AI. No matter if AI is borne from
Silicon Valley hubris or ideological utopianism, cold economic
calculation or pressure to maintain investment, the choice to develop AI
is a human one, one taken to achieve human objectives. It is my hope
that this close examination shows the error of calling AI development
“inevitable”: in every sense of the word, AI development by large
corporate institutions is a human act, one that can be addressed by
reducing the expected reward or increasing the expected downsides
through regulation and enforcement. Fatalism about AI serves the same
purpose it always does, to make us accept without resistance what we
ought to question and scrutinise. Charlie Warzel calls this “AI’s
manifest-destiny philosophy: this is happening, whether you like it or
not”.</em></p>
<p><a href="index.html#toc">Table of Contents</a>
<a href="3.html">Previous Section</a>
<a href="5.html">Next Section</a></p>
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>What is “relevant”, of course, depends on the problem and the
user.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>For an overview of this area, see this MIT course (Computation
Structures, 6.004, 2007):
<u><u><a href="https://www.youtube.com/playlist?list=PLUl4u3cNGP62WVs95MNq3dQBqY2vGOtQ2">https://www.youtube.com/playlist?list=PLUl4u3cNGP62WVs95MNq3dQBqY2vGOtQ2</a></u></u>&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>I am conscious that I am ignoring the setup of Reinforcement
Learning based models, but I believe that even the training regimens
of RL-based models are somewhat isomorphic to this principle.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>Also known as a representation in latent space/embedding space.&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p>In terms of information theory, a 256*256 image with three colour
channels (~20000 bits of information by my napkin math) gets reduced
by an ImageNet classifier to one of 1000 possible classes (~10 bits
of information).&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p>The general term for this research area is “model unlearning”.&#160;<a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p>The exceptional malicious prompts, known colloquially as
“jailbreaks”, are somewhat obscure and generally not the sort of
thing you would type in by accident, but they do exist and are
constantly being found. Here are some examples:
<u><u><a href="https://github.com/elder-plinius/L1B3RT45">https://github.com/elder-plinius/L1B3RT45</a></u></u>&#160;<a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p>In a similar vein, I don’t care if AIs are maliciously extending
their own runtimes, or merely doing so out of naive optimisation. In
general, I am not super concerned as to whether the terminator drone
that kills me might actually have a rich inner life, so long as
bullets are still being fired in the direction of my torso.&#160;<a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:9">
<p>Naturally, I am particularly interested in knowing if any mistakes
have been made in drawing up this benchmark.&#160;<a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
</ol>
</div>
</body>
</html>